{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XJG5shp08CGC"},"source":["Uses Fine-Tuned BERT network to classify biomechanics papers from PubMed"]},{"cell_type":"code","metadata":{"id":"EnZ8CI-rwNOY","executionInfo":{"status":"ok","timestamp":1681418048330,"user_tz":240,"elapsed":665,"user":{"displayName":"Jereme Outerleys","userId":"15452485930552153510"}},"outputId":"b7589615-ef87-42d9-b914-5f9749cb03de","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Check date\n","!rm /etc/localtime\n","#!ln -s /usr/share/zoneinfo/America/Los_Angeles /etc/localtime\n","!ln -s /usr/share/zoneinfo/Canada/Eastern /etc/localtime\n","!date\n","# might need to restart runtime if timezone didn't change\n","# test\n"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Thu 13 Apr 2023 04:34:08 PM EDT\n"]}]},{"cell_type":"code","metadata":{"id":"6klLdde-78nI"},"source":["## Install & load libraries\n","#!pip install tensorflow\n","import tensorflow as tf\n","print(tf. __version__)\n","try:\n","  from Bio import Entrez\n","except:\n","  !pip install -q -U biopython\n","  from Bio import Entrez\n","try:\n","  import tensorflow_text as text\n","except:\n","  !pip install -q -U tensorflow_text\n","  import tensorflow_text as text\n","import numpy as np\n","import pandas as pd\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","import tensorflow as tf  # probably have to lock version\n","import string\n","import datetime\n","from bs4 import BeautifulSoup\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.models import load_model\n","import tensorflow_hub as hub\n","from google.colab import drive\n","import datetime as dt\n","\n","# Mount Google Drive for model and csv up/download\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Define date range\n","today = dt.date.today()\n","\n","yesterday = today - dt.timedelta(days=0) #if doing on Thursday...\n","#days_ago_6 = yesterday - dt.timedelta(days=6) # for text output\n","\n","\n","#yesterday = today - dt.timedelta(days=1) #re-align dates if doing on Friday...\n","#days_ago_6 = yesterday - dt.timedelta(days=6) # for text output\n","\n","#yesterday = today - dt.timedelta(days=2) #re-align dates if doing on Saturday...\n","#yesterday = today - dt.timedelta(days=3) #re-align dates if doing on Sunday...\n","#yesterday = today - dt.timedelta(days=4) #re-align dates if doing on Monday...\n","#yesterday = today - dt.timedelta(days=5) #re-align dates if doing on Tuesday...\n","#yesterday = today - dt.timedelta(days=6) #re-align dates if doing on Wednesday...\n","#yesterday = today - dt.timedelta(days=7) #re-align dates if doing on Thursday...\n","#yesterday = today - dt.timedelta(days=8) #re-align dates if doing on Friday...\n","#yesterday = today - dt.timedelta(days=9) #re-align dates if doing on Saturday...\n","#yesterday = today - dt.timedelta(days=10) #re-align dates if doing on Sunday...\n","\n","#week_ago = yesterday - dt.timedelta(days=7)  # ensure overlap in pubmed search\n","#days_ago_6 = yesterday - dt.timedelta(days=6) # for text output\n","\n","# 2 weeks behind\n","week_ago = yesterday - dt.timedelta(days=14)  # ensure overlap in pubmed search\n","days_ago_6 = yesterday - dt.timedelta(days=13) # for text output\n","\n","print('today: {}'.format(today))\n","print('week_ago (i.e. Start Date):  {}'.format(week_ago))\n","print('days_ago_6 (for text output): {}'.format(days_ago_6))\n","print('yesterday (i.e. End Date): {}'.format(yesterday))\n"],"metadata":{"id":"xpzq3oXFeRyb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define Search Criteria ----\n","def search(query):\n","    Entrez.email = 'your.email@example.com'\n","    handle = Entrez.esearch(db='pubmed',\n","                            #sort='most recent',\n","                            sort='pub date',\n","                            retmax='5000',\n","                            retmode='xml',\n","                            datetype='pdat',  # pdat is published date, edat is entrez date. \n","                            #reldate=7,  # only within n days from now\n","                            mindate= min_date,\n","                            maxdate= max_date,  # for searching date range\n","                            term=query)\n","    results = Entrez.read(handle)\n","    return results\n","\n","\n","# Perform Search and Pull Paper Titles ----\n","def fetch_details(ids):\n","    Entrez.email = 'your.email@example.com'\n","    handle = Entrez.efetch(db='pubmed',\n","                           retmode='xml',\n","                           id=ids)\n","    results = Entrez.read(handle)\n","    return results\n","\n","\n","# Make the stop words for string cleaning ----\n","def html_strip(text):\n","    text = BeautifulSoup(text, 'lxml').text\n","    text = text.replace('[','').replace(']','')\n","    return text\n","\n","def clean_str(text, stops):\n","    text = BeautifulSoup(text, 'lxml').text\n","    text = text.split()\n","    return ' '.join([word for word in text if word not in stops])\n","\n","stop = list(stopwords.words('english'))\n","stop_c = [string.capwords(word) for word in stop]\n","for word in stop_c:\n","    stop.append(word)\n","\n","new_stop = ['The', 'An', 'A', 'Do', 'Is', 'In', 'StringElement', \n","            'NlmCategory', 'Label', 'attributes', 'INTRODUCTION',\n","            'METHODS', 'BACKGROUND', 'RESULTS', 'CONCLUSIONS']\n","for s in new_stop:\n","    stop.append(s)\n","\n","# Search terms (can test string with Pubmed Advanced Search) ----\n","# search_results = search('(Biomech*[Title/Abstract] OR locomot*[Title/Abstract])')\n","\n","#min_date = week_ago.strftime('%m/%d/%Y')\n","min_date = week_ago.strftime('%Y/%m/%d')\n","#max_date = yesterday.strftime('%m/%d/%Y')\n","max_date = yesterday.strftime('%Y/%m/%d')\n"],"metadata":{"id":"WfOCmJpgD4Bn"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JaSczx4IuQ0Y"},"source":["search_results = search('(biomech*[Title/Abstract] OR locomot*[Title/Abstract] NOT opiod*[Title/Abstract] NOT pharm*[Journal] NOT mouse[Title/Abstract] NOT drosophil*[Title/Abstract] NOT mice[Title/Abstract] NOT rats*[Title/Abstract] NOT elegans[Title/Abstract])')\n","id_list = search_results['IdList']\n","papers = fetch_details(id_list)\n","print(len(papers['PubmedArticle']), 'Papers found')\n","\n","titles, full_titles, keywords, authors, links, journals, abstracts = ([] for i in range(7))\n","\n","for paper in papers['PubmedArticle']:\n","    # clean and store titles, abstracts, and links\n","    t = clean_str(paper['MedlineCitation']['Article']['ArticleTitle'], \n","                  stop).replace('[','').replace(']','').capitalize()  # rm brackets that survived beautifulsoup, sentence case\n","    titles.append(t)\n","    full_titles.append(paper['MedlineCitation']['Article']['ArticleTitle'])\n","    pmid = paper['MedlineCitation']['PMID']\n","    links.append('[URL=\"https://www.ncbi.nlm.nih.gov/pubmed/{0}\"]{1}[/URL]'.format(pmid, html_strip(paper['MedlineCitation']['Article']['ArticleTitle'])))\n","    try:\n","        abstracts.append(clean_str(paper['MedlineCitation']['Article']['Abstract']['AbstractText'][0], \n","                                    stop).replace('[','').replace(']','').capitalize())  # rm brackets that survived beautifulsoup, sentence case\n","    except:\n","        abstracts.append('')\n","\n","    # clean and store authors\n","    auths = []\n","    try:\n","        for auth in paper['MedlineCitation']['Article']['AuthorList']:\n","            try:  # see if there is a last name and initials\n","                auth_name = [auth['LastName'], auth['Initials'] + ',']\n","                auth_name = ' '.join(auth_name)\n","                auths.append(auth_name)\n","            except:\n","                if 'LastName' in auth.keys():  # maybe they don't have initials\n","                    auths.append(auth['LastName'] + ',')\n","                else:  # no last name\n","                    auths.append('')\n","                    print(paper['MedlineCitation']['Article']['ArticleTitle'],\n","                          'has an issue with an author name:')\n","\n","    except:\n","        auths.append('AUTHOR NAMES ERROR')\n","        print(paper['MedlineCitation']['Article']['ArticleTitle'], 'has no author list?')\n","    # compile authors\n","    authors.append(' '.join(auths).replace('[','').replace(']',''))  # rm brackets in names\n","    # journal names\n","    journals.append(paper['MedlineCitation']['Article']['Journal']['Title'].replace('[','').replace(']',''))  # rm brackets\n","\n","    # store keywords \n","    if paper['MedlineCitation']['KeywordList'] != []:\n","        kwds = []\n","        for kw in paper['MedlineCitation']['KeywordList'][0]:\n","            kwds.append(kw[:])\n","        keywords.append(', '.join(kwds).lower())\n","    else:\n","      keywords.append('')\n","\n","# Put Titles, Abstracts, Authors, Journal, and Keywords into dataframe\n","papers_df = pd.DataFrame({'title': titles,\n","                          'keywords': keywords,\n","                          'abstract': abstracts,\n","                          'authors': authors,\n","                          'journal': journals,\n","                          'links': links,\n","                          'raw_title': full_titles,\n","                          'mindate': min_date,\n","                          'maxdate': max_date})\n","\n","\n","# remove papers with no title or no authors\n","for index, row in papers_df.iterrows():\n","    if row['title'] == '' or row['authors'] == 'AUTHOR NAMES ERROR':\n","        papers_df.drop(index, inplace=True)\n","papers_df.reset_index(drop=True, inplace=True)\n","\n","# join titles and abstract\n","papers_df['BERT_input'] = pd.DataFrame(papers_df['title'] + ' ' + papers_df['abstract'])\n","\n","# Load Fine-Tuned BERT Network ----\n","model = tf.saved_model.load('/content/gdrive/My Drive/BiomchBERT/Data/BiomchBERT/')\n","print('Loaded model from disk')\n","\n","# Load Label Encoder ----\n","le = LabelEncoder()\n","le.classes_ = np.load('/content/gdrive/My Drive/BiomchBERT/Data/BERT_label_encoder.npy')\n","print('Loaded Label Encoder')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"55tVgCM--ktr"},"source":["# Predict Paper Topic ----\n","predicted_topic = model(papers_df['BERT_input'], training=False)  # will run out of GPU memory (14GB) if predicting more than ~2000 title+abstracts at once"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QvD9JQj7-2_e"},"source":["# Determine Publications that BiomchBERT is unsure about ----\n","topics, pred_val_str = ([] for i in range(2))\n","\n","for pred_prob in predicted_topic:\n","    pred_val = np.max(pred_prob)\n","    if pred_val > 1.5 * np.sort(pred_prob)[-2]:  # Is top confidence score more than 1.5x the second best confidence score?\n","        topics.append(le.inverse_transform([np.argmax(pred_prob)])[0])\n","        top1 = le.inverse_transform([np.argmax(pred_prob)])[0]\n","        top2 = le.inverse_transform([list(pred_prob).index([np.sort(pred_prob)[-2]])])[0]\n","        # pred_val_str.append(pred_val * 100)  # just report top category\n","        pred_val_str.append(str(np.round(pred_val * 100, 1)) + '% ' + str(top1) + '; ' + str(\n","            np.round(np.sort(pred_prob)[-2] * 100, 1)) + '% ' + str(top2))  # report top 2 categories\n","    else:\n","        topics.append('UNKNOWN')\n","        top1 = le.inverse_transform([np.argmax(pred_prob)])[0]\n","        top2 = le.inverse_transform([list(pred_prob).index([np.sort(pred_prob)[-2]])])[0]\n","        pred_val_str.append(str(np.round(pred_val * 100, 1)) + '% ' + str(top1) + '; ' + str(\n","            np.round(np.sort(pred_prob)[-2] * 100, 1)) + '% ' + str(top2))\n","        \n","papers_df['topic'] = topics\n","papers_df['pred_val'] = pred_val_str\n","\n","print('BiomchBERT is unsure about {0} papers\\n'.format(len(papers_df[papers_df['topic'] == 'UNKNOWN'])))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eeacO2b3WHFu"},"source":["# Prompt User to decide for BiomchBERT ----\n","unknown_papers = papers_df[papers_df['topic'] == 'UNKNOWN']\n","for indx, paper in unknown_papers.iterrows():\n","  print(paper['raw_title'])\n","  print(paper['journal'])\n","  print(paper['pred_val'])\n","  print()\n","  splt_str = paper['pred_val'].split(';')\n","  options = [str for pred_cls in splt_str for str in le.classes_ if (str in pred_cls)]\n"," \n"," \n","  choice = input('(1)st topic, (2)nd topic, (o)ther topic, or (r)emove paper? ')\n","  print()\n","  if choice == '1':\n","    papers_df.iloc[indx]['topic'] = str(options[0])\n","  elif choice == '2':\n","    papers_df.iloc[indx]['topic'] = str(options[1])\n","  elif choice == 'o':\n","    # print all categories so you can select\n","    for i in zip(range(len(le.classes_)),le.classes_):\n","      print(i)  \n","    new_cat = input('Enter number of new class or type \"r\" to remove paper: ')\n","    print()\n","    if new_cat == 'r':\n","      papers_df.iloc[indx]['topic'] = '_REMOVE_'  # not deleted, but withheld from text file output\n","    else:\n","      papers_df.iloc[indx]['topic'] = le.classes_[int(new_cat)] \n","  elif choice == 'r':\n","    papers_df.iloc[indx]['topic'] = '_REMOVE_'  # not deleted, but withheld from text file output\n"," \n","print('Removing {0} papers\\n'.format(len(papers_df[papers_df['topic'] == '_REMOVE_'])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Deqf5q7BdTAJ"},"source":["# Double check that none of these papers were included in past literature updates ----\n","# load prior papers\n","# papers_df.to_csv('/content/gdrive/My Drive/BiomchBERT/Updates/prior_papers.csv', index=False)  # run ONLY if there are no prior papers\n","prior_papers = pd.read_csv('/content/gdrive/My Drive/BiomchBERT/Updates/prior_papers.csv')\n","prior_papers.dropna(subset=['title'], inplace=True)\n","prior_papers.reset_index(drop=True, inplace=True)\n","\n","# NEED TO DO: find matching papers between current week and prior papers using Pubmed ID since titles can change from ahead of print to final version.\n","# match = papers_df['links'].split(']')[0].isin(prior_papers['links'].split(']')[0])\n","\n","match = papers_df['title'].isin(prior_papers['title'])  # boolean\n","print('Removing {0} papers found in prior literature updates\\n'.format(sum(match)))\n","# filter and check if everything accidentally was removed\n","filtered_papers_df = papers_df.drop(papers_df[match].index)\n","if filtered_papers_df.shape[0] < 1:\n","    raise ValueError('might have removed all the papers for some reason. ')\n","else:\n","    papers_df = filtered_papers_df\n","    papers_df.reset_index(drop=True, inplace=True)\n","    updated_prior_papers = pd.concat([prior_papers, papers_df], axis=0)\n","    updated_prior_papers.reset_index(drop=True, inplace=True)\n","    updated_prior_papers.to_csv('/content/gdrive/My Drive/BiomchBERT/Updates/prior_papers.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fQKyxoBF36B"},"source":["# Create Text File for Biomch-L ----\n","# Compile papers grouped by topic\n","txtname = '/content/gdrive/My Drive/BiomchBERT/Updates/' + today.strftime(\"%Y-%m-%d\") + '-litupdate.txt'\n","txt = open(txtname, 'w', encoding='utf-8')\n","txt.write('[SIZE=16px][B]LITERATURE UPDATE[/B][/SIZE]\\n')\n","txt.write(days_ago_6.strftime(\"%b %d, %Y\") + ' - '+ yesterday.strftime(\"%b %d, %Y\")+'\\n')  # a week ago from yesterday.\n","txt.write(\n","    \"\"\"\n","Literature search terms: biomech* & locomot*\n","\n","Publications are classified by [URL=\"https://www.ryan-alcantara.com/projects/p88_BiomchBERT/\"]BiomchBERT[/URL], a neural network trained on past Biomch-L Literature Updates. BiomchBERT is managed by [URL=\"https://jouterleys.github.io\"]Jereme Outerleys[/URL], a Doctoral Student at Queen's University. Each publication has a score (out of 100%) reflecting how confident BiomchBERT is that the publication belongs in a particular category (top 2 shown). If something doesn't look right, email jereme.outerleys[at]queensu.ca.\n","\n","Twitter: [URL=\"https://www.twitter.com/jouterleys\"]@jouterleys[/URL]. \n","\n","\n","    \"\"\"\n","    )\n","\n","# Write papers to text file grouped by topic ----\n","topic_list = np.unique(papers_df.sort_values('topic')['topic'])\n","\n","for topic in topic_list:\n","    papers_subset = pd.DataFrame(papers_df[papers_df.topic == topic].reset_index(drop=True))\n","    txt.write('\\n')\n","    # TOPIC NAME (with some cleaning)\n","    if topic == '_REMOVE_':\n","      continue\n","    elif topic == 'UNKNOWN':\n","        txt.write('[SIZE=16px][B]*Papers BiomchBERT is unsure how to classify*[/B][/SIZE]\\n')\n","    elif topic == 'CARDIOVASCULAR/CARDIOPULMONARY':\n","      topic = 'CARDIOVASCULAR/PULMONARY'\n","      txt.write('[SIZE=16px][B]*%s*[/B][/SIZE]\\n' % topic)\n","    elif topic == 'CELLULAR/SUBCELLULAR':\n","      topic = 'CELLULAR'\n","      txt.write('[SIZE=16px][B]*%s*[/B][/SIZE]\\n' % topic)\n","    elif topic == 'ORTHOPAEDICS/SURGERY':\n","      topic = 'ORTHOPAEDICS (SURGERY)'\n","      txt.write('[SIZE=16px][B]*%s*[/B][/SIZE]\\n' % topic)\n","    elif topic == 'ORTHOPAEDICS/SPINE':\n","      topic = 'ORTHOPAEDICS (SPINE)'\n","      txt.write('[SIZE=16px][B]*%s*[/B][/SIZE]\\n' % topic)\n","    else:\n","        txt.write('[SIZE=16px][B]*%s*[/B][/SIZE]\\n' % topic)\n","    # HYPERLINKED PAPERS, AUTHORS, JOURNAL NAME\n","    for i, paper in enumerate(papers_subset['links']):\n","        txt.write('[B]%s[/B] ' % paper)\n","        txt.write('%s ' % papers_subset['authors'][i])\n","        txt.write('[I]%s[/I]. ' % papers_subset['journal'][i])\n","        # CONFIDENCE SCORE (BERT softmax categorical crossentropy)\n","        try:\n","            txt.write('(%.1f%%) \\n\\n' % papers_subset['pred_val'][i])\n","        except:\n","            txt.write('(%s)\\n\\n' % papers_subset['pred_val'][i]) \n","\n","txt.write('[SIZE=16px][B]*PICK OF THE WEEK*[/B][/SIZE]\\n')\n","txt.close()\n","print('Literature Update Exported for Biomch-L')\n","print('Location:', txtname)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Sometimes there are invalid characters to paste in html on biomechl\n","# Use something like https://www.textfixer.com/html/html-character-encoding.php to encode these characters properly"],"metadata":{"id":"McR_o-iu7Czf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#txt_file = 'C:/Users/Blast/Downloads/2023-03-10-litupdate.txt'\n","#encoded_file = txt_file.replace('.txt', '-encoded.txt')\n","# Open the input file with the correct encoding\n","#with open(txt_file, 'r', encoding='utf-8') as file:\n","    # Read the contents of the file\n","    #contents = file.read()\n","    \n","#encoded_contents = contents.encode('ascii', 'xmlcharrefreplace')\n","\n","# Open the output file with the correct encoding\n","#with open(encoded_file, 'wb') as file:\n","    # Write the escaped output to the file\n","    #file.write(encoded_contents)"],"metadata":{"id":"cQTB3vVCOdXE"},"execution_count":null,"outputs":[]}]}